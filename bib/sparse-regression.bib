Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@book{MacKay2003,
author = {MacKay, David J.C.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/MacKay/Unknown/MacKay - 2003 - Information Theory, Inference, and Learning Algorithms.pdf:pdf},
isbn = {0521642981},
keywords = {HMC,Hamiltonian,Monte Carlo},
mendeley-tags = {HMC,Hamiltonian,Monte Carlo},
publisher = {Cambridge University Press},
title = {{Information Theory, Inference, and Learning Algorithms}},
year = {2003}
}
@book{Neal2011,
abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard to compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories form taking much computation time.},
archivePrefix = {arXiv},
arxivId = {1206.1901},
author = {Neal, Radford M.},
booktitle = {Handbook of Markov Chain Monte Carlo},
doi = {doi:10.1201/b10905-6},
eprint = {1206.1901},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Neal/Handbook of Markov Chain Monte Carlo/Neal - 2011 - MCMC using Hamiltonian dynamics.pdf:pdf},
isbn = {9781420079418},
issn = {{\textless}null{\textgreater}},
keywords = {hamiltonian dynamics,mcmc},
pages = {113--162},
pmid = {25246403},
title = {{MCMC using Hamiltonian dynamics}},
year = {2011}
}
@article{wainwright2009sharp,
author = {Wainwright, Martin J},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Wainwright/IEEE transactions on information theory/Wainwright - 2009 - Sharp thresholds for high-dimensional and noisy sparsity recovery using-constrained quadratic programming (Lasso).pdf:pdf},
journal = {IEEE transactions on information theory},
number = {5},
pages = {2183--2202},
publisher = {IEEE},
title = {{Sharp thresholds for high-dimensional and noisy sparsity recovery using-constrained quadratic programming (Lasso)}},
volume = {55},
year = {2009}
}
@article{Tibshirani2007,
abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
archivePrefix = {arXiv},
arxivId = {1369–7412/11/73273},
author = {Tibshirani, Robert},
doi = {10.1111/j.1467-9868.2011.00771.x},
eprint = {11/73273},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Tibshirani/Journal of the Royal Statistical Society. Series B Statistical Methodology/Tibshirani - 2007 - Regression Shrinkage and Selection via the Lasso.pdf:pdf},
isbn = {1467-9868},
issn = {0035-9246},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {1-penalty,Penalization,Regularization,l},
number = {1},
pages = {267--288},
pmid = {1000198927},
primaryClass = {1369–7412},
title = {{Regression Shrinkage and Selection via the Lasso}},
volume = {58},
year = {2007}
}
@article{akaike1974new,
author = {Akaike, Hirotugu},
journal = {IEEE transactions on automatic control},
number = {6},
pages = {716--723},
publisher = {Ieee},
title = {{A new look at the statistical model identification}},
volume = {19},
year = {1974}
}
@article{Bickel2009,
abstract = {We show that, under a sparsity scenario, the Lasso estimator and the Dantzig selector exhibit similar behavior. For both methods, we derive, in par-allel, oracle inequalities for the prediction risk in the general nonparametric regression model, as well as bounds on the p estimation loss for 1 ≤ p ≤ 2 in the linear model when the number of variables can be much larger than the sample size.},
archivePrefix = {arXiv},
arxivId = {0801.1095},
author = {Bickel, Peter J. and Ritov, Y. and Tsybakov, Alexandre B.},
doi = {10.1214/08-AOS620},
eprint = {0801.1095},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bickel, Ritov, Tsybakov/Annals of Statistics/Bickel, Ritov, Tsybakov - 2009 - Simultaneous analysis of lasso and dantzig selector.pdf:pdf},
isbn = {0090-5364},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Linear models,Model selection,Nonparametric statistics},
number = {4},
pages = {1705--1732},
title = {{Simultaneous analysis of lasso and dantzig selector}},
volume = {37},
year = {2009}
}
@article{Schwarz1978,
abstract = {The problem of selecting one of a number of models of different dimensions is treated by finding its Bayes solution, and evaluating the leading terms of its asymptotic expansion. These terms are a valid large-sample criterion beyond the Bayesian context, since they do not depend on the a priori distribution.},
author = {Schwarz, Gideon},
doi = {10.1214/aos/1176344136},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Schwarz/The Annals of Statistics/Schwarz - 1978 - Estimating the dimension of a model.pdf:pdf},
isbn = {0780394224},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {2},
pages = {461--464},
pmid = {2958889},
title = {{Estimating the dimension of a model}},
url = {http://projecteuclid.org/euclid.aos/1176344136},
volume = {6},
year = {1978}
}
@article{Tseng2001,
abstract = {We study the convergence properties of a (block) coordinate descent method applied to minimize a nondifferentiable (nonconvex) function f (x1 , . . . , xN ) with certain separability and regularity proper- ties. Assuming that f is continuous on a compact level set, the sub- sequence convergence of the iterates to a stationary point is shown when either f is pseudoconvex in every pair of coordinate blocks from among NA1 coordinate blocks or f has at most one minimum in each of NA2 coordinate blocks. If f is quasiconvex and hemivariate in every coordi- nate block, then the assumptions of continuity of f and compactness of the level set may be relaxed further. These results are applied to derive new (and old) convergence results for the proximal minimization algo- rithm, an algorithm of Arimoto and Blahut, and an algorithm of Han. They are applied also to a problem of blind source separation.},
author = {Tseng, P.},
doi = {10.1023/A:1017501703105},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Tseng/Journal of Optimization Theory and Applications/Tseng - 2001 - Convergence of a block coordinate descent method for nondifferentiable minimization.pdf:pdf},
isbn = {0022-3239},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Block coordinate descent,Convergence,Gauss-seidel method,Nondifferentiable minimization,Pseudoconvex functions,Quasiconvex functions,Stationary point},
number = {3},
pages = {475--494},
title = {{Convergence of a block coordinate descent method for nondifferentiable minimization}},
volume = {109},
year = {2001}
}
