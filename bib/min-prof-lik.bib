Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{September2008,
abstract = {The lme4 package provides R functions to fit and analyze several different types of mixed-effects models, including linear mixed models, generalized linear mixed models and nonlinear mixed models. In this vignette we describe the formulation of these models and the compu- tational approach used to evaluate or approximate the log-likelihood of a model/data/parameter value combination.},
author = {Bates, Douglas M.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bates/Unknown/Bates - 2008 - Computational methods for mixed models.pdf:pdf},
title = {{Computational methods for mixed models}},
year = {2008}
}
@article{Blei2016,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability distributions. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation about the posterior. In this paper, we review variational inference (VI), a method from machine learning that approximates probability distributions through optimization. VI has been used in myriad applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of distributions and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this widely-used class of algorithms.},
archivePrefix = {arXiv},
arxivId = {1601.00670},
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
eprint = {1601.00670},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Blei, Kucukelbir, McAuliffe/Unknown/Blei, Kucukelbir, McAuliffe - 2016 - Variational Inference A Review for Statisticians.pdf:pdf},
keywords = {Graphical Model,Variational Inference},
title = {{Variational Inference: A Review for Statisticians}},
year = {2016}
}
@book{davis2006direct,
author = {Davis, Timothy A},
publisher = {Siam},
title = {{Direct methods for sparse linear systems}},
volume = {2},
year = {2006}
}
@article{Duff:2013,
annote = {revision {\#}153309},
author = {Duff, I and U{\c{c}}ar, B},
journal = {Scholarpedia},
number = {10},
pages = {9700},
title = {{Direct methods for sparse matrix solution}},
url = {http://dx.doi.org/10.4249/scholarpedia.9700},
volume = {8},
year = {2013}
}
@article{Bates2014,
abstract = {Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.5823v1},
author = {Bates, Douglas M. and M{\"{a}}chler, Martin and Bolker, Benjamin M and Walker, Steven C},
doi = {10.1177/009286150103500418},
eprint = {arXiv:1406.5823v1},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bates et al/arXiv1406.5823v1stat.CO23/Bates et al. - 2014 - Fitting linear mixed-effects models using lme4.pdf:pdf},
isbn = {{\%}(},
issn = {0092-8615},
journal = {arXiv:1406.5823v1[stat.CO]23},
keywords = {cholesky,linear mixed models,penalized least squares,sparse matrix methods},
pages = {1 -- 51},
title = {{Fitting linear mixed-effects models using lme4}},
year = {2014}
}
@book{Bishop2006,
author = {Bishop, Christopher M.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bishop/Unknown/Bishop - 2006 - Pattern Recognition and Machine Learning.pdf:pdf},
publisher = {Springer},
title = {{Pattern Recognition and Machine Learning}},
year = {2006}
}
