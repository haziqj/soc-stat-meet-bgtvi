Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Kass1995,
author = {Kass, Robert and Raftery, Adrian},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kass, Raftery/Journal of the American Statistical Association/Kass, Raftery - 1995 - Bayes Factors.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {430},
pages = {773--795},
title = {{Bayes Factors}},
volume = {90},
year = {1995}
}
@article{Blei2016,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability distributions. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation about the posterior. In this paper, we review variational inference (VI), a method from machine learning that approximates probability distributions through optimization. VI has been used in myriad applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of distributions and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this widely-used class of algorithms.},
archivePrefix = {arXiv},
arxivId = {1601.00670},
author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
eprint = {1601.00670},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Blei, Kucukelbir, McAuliffe/Unknown/Blei, Kucukelbir, McAuliffe - 2016 - Variational Inference A Review for Statisticians.pdf:pdf},
keywords = {Graphical Model,Variational Inference},
title = {{Variational Inference: A Review for Statisticians}},
year = {2016}
}
@manual{Jamil2017,
address = {R Package version 0.6.4},
annote = {R package version 0.6.4.9003},
author = {\HJ},
publisher = {\href{https://CRAN.R-project.org/package=iprior}{CRAN}},
%url = {https://CRAN.R-project.org/package=iprior},
title = {{iprior: Linear Regression using I-Priors}},
year = {2017}
}
@article{Bergsma2017,
author = {Bergsma, Wicher},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bergsma/Manuscript in preparation/Bergsma - 2017 - Regression with I-priors.pdf:pdf},
journal = {Manuscript in preparation},
title = {{Regression with I-priors}},
year = {2017}
}
@book{Skrondal2004,
abstract = {This book unifies and extends latent variable models, including multilevel or generalized linear mixed models, longitudinal or panel models, item response or factor models, latent class or finite mixture models, and structural equation models. Following a gentle introduction to latent variable modeling, the authors clearly explain and contrast a wide range of estimation and prediction methods from biostatistics, psychometrics, econometrics, and statistics. They present exciting and realistic applications that demonstrate how researchers can use latent variable modeling to solve concrete problems in areas as diverse as medicine, economics, and psychology. The examples considered include many nonstandard response types, such as ordinal, nominal, count, and survival data. Joint modeling of mixed responses, such as survival and longitudinal data, is also illustrated. Numerous displays, figures, and graphs make the text vivid and easy to read.},
author = {Skrondal, Anders and Rabe-Hesketh, Sophia},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Skrondal, Rabe-Hesketh/Unknown/Skrondal, Rabe-Hesketh - 2004 - Generalized Latent Variable Modeling Multilevel, Longitudinal, and Structural Equation Models.pdf:pdf},
publisher = {Chapman {\&} Hall/CRC},
title = {{Generalized Latent Variable Modeling: Multilevel, Longitudinal, and Structural Equation Models}},
year = {2004}
}
@book{Rasmussen2006,
author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Rasmussen, Williams/Unknown/Rasmussen, Williams - 2006 - Gaussian Processes for Machine Learning.pdf:pdf},
isbn = {0-262-18253-X},
publisher = {The MIT Press},
title = {{Gaussian Processes for Machine Learning}},
year = {2006}
}
@book{Murphy1991,
abstract = {Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package—PMTK (probabilistic modeling toolkit)—that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.},
author = {Murphy, Kevin P.},
booktitle = {Machine Learning: A Probabilistic Perspective},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Murphy/Machine Learning A Probabilistic Perspective/Murphy - 2012 - Machine Learning A Probabilistic Perspective.pdf:pdf},
publisher = {The MIT Press},
title = {{Machine Learning: A Probabilistic Perspective}},
year = {2012}
}
@article{goeman2012penalized,
author = {Goeman, Jelle and Meijer, Rosa and Chaturvedi, Nimisha},
title = {{penalized: L1 (lasso and fused lasso) and L2 (ridge) Penalized Estimation in GLMs and in the Cox Model}},
year = {2012}
}
@article{Beal2006,
author = {Beal, M J and Ghahramani, Z},
doi = {DOI:10.1214/06-BA126},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Beal, Ghahramani/Bayesian Analysis/Beal, Ghahramani - 2006 - Variational Bayesian Learning of Directed Graphical Models with Hidden Variables.pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {approximate bayesian inference,bayes factors,directed acyclic,em algorithm,graphical models,graphs,markov chain monte carlo,model,selection,variational bayes},
number = {4},
pages = {793--832},
title = {{Variational Bayesian Learning of Directed Graphical Models with Hidden Variables}},
volume = {1},
year = {2006}
}
@book{Bishop2006,
author = {Bishop, Christopher M.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bishop/Unknown/Bishop - 2006 - Pattern Recognition and Machine Learning.pdf:pdf},
publisher = {Springer},
title = {{Pattern Recognition and Machine Learning}},
year = {2006}
}
@misc{ArrhythmiaData,
author = {Guvenir, H. Altay and {Burak Acar}, M.S. and Muderrisoglu, Haldun},
institution = {University of California, Irvine, School of Information and Computer Sciences},
title = {{UCI Machine Learning Repository: Arrhythmia Data Set}},
url = {https://archive.ics.uci.edu/ml/datasets/Arrhythmia},
year = {1998}
}
@article{Cannings2017,
author = {Cannings, Timothy I and Samworth, Richard J},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Cannings, Samworth/JRSSBd/Cannings, Samworth - 2017 - Random-projection ensemble classification.pdf:pdf},
journal = {\JRSSBd},
keywords = {aggregation,classification,high dimensional classification,random projection},
title = {{Random-projection ensemble classification}},
volume = {to appear},
year = {2017}
}
@book{Jamil2017iprobit,
address = {R Package version 0.1.0},
author = {\HJ},
publisher = {\href{https://github.com/haziqjamil/iprobit}{GitHub}},
title = {{iprobit: Binary Probit Regression with I-Priors}},
year = {2017}
}
@article{tibshirani2003class,
author = {Tibshirani, Robert and Hastie, Trevor and Narasimhan, Balasubramanian and Chu, Gilbert},
journal = {Statistical Science},
number = {1},
pages = {104--117},
title = {{Class prediction by nearest shrunken centroids, with applications to DNA microarrays}},
volume = {18},
year = {2003}
}
