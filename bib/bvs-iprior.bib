Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Dellaportas2002,
abstract = {Several MCMC methods have been proposed for estimating probabilities of models and associated ‘model-averaged' posterior distributions in the presence of model uncertainty.We discuss, compare, develop and illustrate several of these methods, focussing on connections between them.},
author = {Dellaportas, Petros and Forster, Jonathan J. and Ntzoufras, Ioannis},
doi = {10.1023/A:1013164120801},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Dellaportas, Forster, Ntzoufras/Statistics and Computing/Dellaportas, Forster, Ntzoufras - 2002 - On Bayesian model and variable selection using MCMC.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Gibbs sampler,Independence sampler,Metropolis-Hastings,Reversible jump},
number = {1},
pages = {27--36},
title = {{On Bayesian model and variable selection using MCMC}},
volume = {12},
year = {2002}
}
@article{Barbieri2004,
abstract = {Often the goal of model selection is to choose a model for future prediction, and it is natural to measure the accuracy of a future prediction by squared error loss. Under the Bayesian approach, it is commonly perceived that the optimal predictive model is the model with highest posterior probability, but this is not necessarily the case. In this paper we show that, for selection among normal linear models, the optimal predictive model is often the median probability model, which is defined as the model consisting of those variables which have overall posterior probability greater than or equal to 1/2 of being in a model. The median probability model often differs from the highest probability model.},
archivePrefix = {arXiv},
arxivId = {arXiv:math/0406464v1},
author = {Barbieri, Maria Maddalena and Berger, James O},
doi = {10.1214/009053604000000238},
eprint = {0406464v1},
file = {:Users/haziqjamil/Downloads/euclid.aos.1085408489.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bayesian linear models,Predictive distribution,Squared error loss,Variable selection},
number = {3},
pages = {870--897},
primaryClass = {arXiv:math},
title = {{Optimal predictive model selection}},
volume = {32},
year = {2004}
}
@article{Sejdinovic2012,
author = {Sejdinovic, Dino and Gretton, Arthur},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Sejdinovic, Gretton/COMPGI13 Advanced Topics in Machine Learning. Lecture conducted at University College London/Sejdinovic, Gretton - 2012 - Lecture notes What is an RKHS.pdf:pdf},
journal = {COMPGI13 Advanced Topics in Machine Learning. Lecture conducted at University College London},
pages = {1--24},
title = {{Lecture notes: What is an RKHS?}},
url = {http://www.gatsby.ucl.ac.uk/{~}gretton/coursefiles/RKHS{\_}Notes1.pdf},
year = {2012}
}
@article{Fouskakis2008,
abstract = {Traditional variable-selection strategies in generalized linear models (GLMs) seek to optimize a measure of predictive accuracy without regard for the cost of data collection. When the purpose of such model building is the creation of predictive scales to be used in future studies with constrained budgets, the standard approach may not be optimal. We propose a Bayesian decision-theoretic framework for variable selection in binary-outcome GLMs where the budget for data collection is constrained and potential predictors may vary considerably in cost. The method is illustrated using data from a large study of quality of hospital care in the U.S. in the 1980s. Especially when the number of available predictors p is large, it is important to use an appropriate technique for optimization (e.g., in an application presented here where p = 83, the space over which we search has 283 .= 1025 elements, which is too large to explore using brute force enumeration). Specifically, we investigate simulated annealing (SA), genetic algorithms (GAs), and the tabu search (TS) method used in operations research, and we develop a context-specific version of SA, improved simulated annealing (ISA), that performs better than its generic counterpart. When p was modest in our study, we found that GAs performed relatively poorly for all but the very best user-defined input configurations, generic SA did not perform well, and TS had excellent median performance and was much less sensitive to suboptimal choice of user-defined inputs. When p was large in our study, the best versions of GA and ISA outperformed TS and generic SA. Our results are presented in the context of health policy but can apply to other quality assessment settings with dichotomous outcomes as well.},
author = {Fouskakis, Dimitris and Draper, David},
doi = {10.1198/016214508000001048},
file = {:Users/haziqjamil/Downloads/016214508000001048.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {annealing,bayesian decision theory,cross-validation,genetic algorithm,input,logistic regression,maximization,monte carlo methods,of expected utility,output analysis,prediction,quality of health care,sickness at hospital admission,simulated,tabu search,variable selection},
number = {484},
pages = {1367--1381},
title = {{Comparing Stochastic Optimization Methods for Variable Selection in Binary Outcome Prediction, With Application to Health Policy}},
volume = {103},
year = {2008}
}
@article{Petralias2013,
author = {Petralias, Athanassios and Dellaportas, Petros},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Petralias, Dellaportas/Journal of Statistical Computation and Simulation/Petralias, Dellaportas - 2013 - An MCMC model search algorithm for.pdf:pdf},
journal = {Journal of Statistical Computation and Simulation},
keywords = {gene selection,population sampling,reversible jump,tracking portfolio},
number = {9},
pages = {1722--1740},
title = {{An MCMC model search algorithm for}},
url = {https://www.tol-project.org/svn/tolp/OfficialTolArchiveNetwork/ArimaTools/doc/bibliografia/MCMC/An MCMC model search algorithm for.pdf},
volume = {83},
year = {2013}
}
@article{Lee2003,
abstract = {Selection of significant genes via expression patterns is an important problem in microarray experiments. Owing to small sample size and the large number of variables (genes), the selection process can be unstable. This paper proposes a hierarchical Bayesian model for gene (variable) selection. We employ latent variables to specialize the model to a regression setting and uses a Bayesian mixture prior to perform the variable selection. We control the size of the model by assigning a prior distribution over the dimension (number of significant genes) of the model. The posterior distributions of the parameters are not in explicit form and we need to use a combination of truncated sampling and Markov Chain Monte Carlo (MCMC) based computation techniques to simulate the parameters from the posteriors. The Bayesian model is flexible enough to identify significant genes as well as to perform future predictions. The method is applied to cancer classification via cDNA microarrays where the genes BRCA1 and BRCA2 are associated with a hereditary disposition to breast cancer, and the method is used to identify a set of significant genes. The method is also applied successfully to the leukemia data. SUPPLEMENTARY INFORMATION: http://stat.tamu.edu/people/faculty/bmallick.html.},
author = {Lee, Kyeong Eun and Sha, Naijun and Dougherty, Edward R and Vannucci, Marina and Mallick, Bani K},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Lee et al/Bioinformatics (Oxford, England)/Lee et al. - 2003 - Gene selection a Bayesian variable selection approach.pdf:pdf},
isbn = {1367-4803 LA - eng PT - Evaluation Studies PT - Journal Article PT - Validation Studies},
issn = {1367-4803},
journal = {Bioinformatics (Oxford, England)},
number = {1},
pages = {90--97},
pmid = {12499298},
title = {{Gene selection: a Bayesian variable selection approach}},
volume = {19},
year = {2003}
}
@book{berlinet2011,
author = {Berlinet, Alain and Thomas-Agnan, Christine},
publisher = {Springer Science {\&} Business Media},
title = {{Reproducing kernel Hilbert spaces in probability and statistics}},
year = {2011}
}
@book{Ntzoufras2008,
author = {Ntzoufras, Ioannis},
booktitle = {Wiley},
doi = {10.1002/9780470434567.ch11},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Ntzoufras/Wiley/Ntzoufras - 2011 - Bayesian Modeling Using WinBUGS.pdf:pdf},
keywords = {Bayes factors,Monte Carlo estimators,harmonic mean estimators,marginal likelihood,prior predictive distributions},
pages = {389--433},
publisher = {Wiley},
title = {{Bayesian Modeling Using WinBUGS}},
year = {2011}
}
@article{Zellner1986,
author = {Zellner, Arnold},
file = {:Users/haziqjamil/Downloads/Zellner-1986.pdf:pdf},
isbn = {0444877126},
journal = {Bayesian Inference and Decision Techniques: Essays in Honor of Bruno de Finetti},
keywords = {Arnold,Zellner},
pages = {233--243},
title = {{On assessing prior distributions and Bayesian regresison analysis with g-prior distributions}},
year = {1986}
}
@article{Casella2006,
abstract = {A novel fully automatic Bayesian procedure for variable selection in normal regression models is proposed. The procedure uses the posterior probabilities of the models to drive a stochastic search. The posterior probabilities are computed using intrinsic priors, which can be considered default priors for model selection problems; that is, they are derived from the model structure and are free from tuning parameters. Thus they can be seen as objective priors for variable selection. The stochastic search is based on a Metropolis–Hastings algorithm with a stationary distribution proportional to the model posterior probabilities. The procedure is illustrated on both simulated and real examples.},
author = {Casella, George and Moreno, El{\'{i}}as},
doi = {10.1198/016214505000000646},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Casella, Moreno/Journal of the American Statistical Association/Casella, Moreno - 2006 - Objective Bayesian Variable Selection.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hastings algorithm,intrinsic prior,methods,metropolis,monte carlo markov chain,normal linear regression},
number = {473},
pages = {157--167},
title = {{Objective Bayesian Variable Selection}},
volume = {101},
year = {2006}
}
@article{Jaynes1957a,
abstract = {Treatment of the predictive aspect of statistical mechanics as a form of statistical inference is extended to the density-matrix formalism and applied to a discussion of the relation between irreversibility and information loss. A principle of "statistical complementarity" is pointed out, according to which the empirically verifiable probabilities of statistical mechanics necessarily correspond to incomplete predictions. A preliminary discussion is given of the second law of thermodynamics and of a certain class of irreversible processes, in an approximation equivalent to that of the semiclassical theory of radiation.$\backslash$nIt is shown that a density matrix does not in general contain all the information about a system that is relevant for predicting its behavior. In the case of a system perturbed by random fluctuating fields, the density matrix cannot satisfy any differential equation because $\rho$̇(t) does not depend only on $\rho$(t), but also on past conditions The rigorous theory involves stochastic equations in the type $\rho$(t)=G(t, 0)$\rho$(0), where the operator G is a functional of conditions during the entire interval (0→t). Therefore a general theory of irreversible processes cannot be based on differential rate equations corresponding to time-proportional transition probabilities. However, such equations often represent useful approximations.},
author = {Jaynes, Edwin T.},
doi = {10.1103/PhysRev.108.171},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jaynes/Physical Review/Jaynes - 1957 - Information theory and statistical mechanics II.pdf:pdf},
isbn = {1536-6065},
issn = {0031899X},
journal = {Physical Review},
number = {2},
pages = {171--190},
pmid = {17798674},
title = {{Information theory and statistical mechanics II}},
volume = {108},
year = {1957}
}
@article{Breiman1985,
author = {Breiman, L and Friedman, J H},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Breiman, Friedman/Journal of the American Statistical Association/Breiman, Friedman - 1985 - Estimating optimal transformations for multiple-regression and correlation.pdf:pdf},
journal = {Journal of the American Statistical Association},
keywords = {smoothing},
number = {391},
pages = {614--619},
title = {{Estimating optimal transformations for multiple-regression and correlation}},
volume = {80},
year = {1985}
}
@article{Bergsma2014,
author = {Bergsma, Wicher P},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bergsma/Unpublished/Bergsma - 2016 - Objective Bayes regression with I-priors.pdf:pdf;:Users/haziqjamil/Documents/Mendeley Desktop/Bergsma/Unpublished/Bergsma - 2016 - Objective Bayes regression with I-priors(2).pdf:pdf},
journal = {[Unpublished]},
title = {{Objective Bayes regression with I-priors}},
year = {2016}
}
@article{Kuo1998,
author = {Kuo, L and Mallick, B},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kuo, Mallick/Sankhya The Indian Journal of Statistics, Series B/Kuo, Mallick - 1998 - Variable selection for regression models.pdf:pdf},
journal = {Sankhya: The Indian Journal of Statistics, Series B},
number = {1},
pages = {65--81},
title = {{Variable selection for regression models}},
volume = {60},
year = {1998}
}
@article{Ntzoufras2002,
abstract = {In this paper we discuss and present in detail the implementation of Gibbs variable selection as defined by Dellaportas et al. (2000, 2002) using the BUGS software (Spiegelhalter et al. ,'96a,b,c). The specification of the likelihood, prior and pseudo-prior distributions of the parameters as well as the prior term and model probabilities are described in detail. Guidance is also provided for the calculation of the posterior probabilities within BUGS environment when the number of models is limited. We illustrate the application of this methodology in a variety of problems including linear regression, log-linear and binomial response models.},
author = {Ntzoufras, Ioannis},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Ntzoufras/Journal of Statistical Software/Ntzoufras - 2002 - Gibbs Variable Selection using BUGS.pdf:pdf},
issn = {{\textless}null{\textgreater}},
journal = {Journal of Statistical Software},
keywords = {linear regression,logistic regression,mcmc,model selec-},
number = {i07},
pages = {1--19},
title = {{Gibbs Variable Selection using BUGS}},
url = {http://ideas.repec.org/a/jss/jstsof/07i07.html{\%}5Cnpapers2://publication/uuid/4A46EAD4-E5FD-4634-A6CE-4BC64491A691},
volume = {07},
year = {2002}
}
@techreport{Hein2004,
abstract = {This paper gives a survey of results in the mathematical literature on positive definite kernels and their associated structures. We concentrate on properties which seem potentially relevant for Machine Learning and try to clarify some results that have been misused in the literature. Moreover we consider different lines of generalizations of positive definite kernels. Namely we deal with operator-valued kernels and present the general framework of Hilbertian subspaces of Schwartz which we use to introduce kernels which are distributions. Finally indefinite kernels and their associated reproducing kernel spaces are considered.},
author = {Hein, Matthias and Bousquet, Olivier},
booktitle = {Max–Planck–Institut f{\"{u}}r biologische Kybernetik},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Hein, Bousquet/Max–Planck–Institut f{\"{u}}r biologische Kybernetik/Hein, Bousquet - 2004 - Kernels, associated structures and generalizations.pdf:pdf},
number = {127},
title = {{Kernels, associated structures and generalizations}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.9510{\&}rep=rep1{\&}type=pdf{\%}5Cnpapers2://publication/uuid/509B266F-E508-4031-8519-68B8FCB109CA},
year = {2004}
}
@book{steinwart2008,
author = {Steinwart, Ingo and Christmann, Andreas},
publisher = {Springer Science {\&} Business Media},
title = {{Support vector machines}},
year = {2008}
}
@article{OHara2009,
author = {O'Hara, R B and Sillanp{\"{a}}{\"{a}}, M J},
doi = {10.1214/09-BA403},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/O'Hara, Sillanp{\"{a}}{\"{a}}/Bayesian Analysis/O'Hara, Sillanp{\"{a}}{\"{a}} - 2009 - A review of Bayesian variable selection methods what, how and which.pdf:pdf},
issn = {1936-0975},
journal = {Bayesian Analysis},
keywords = {bugs,mcmc,variable selection},
number = {1},
pages = {85--117},
title = {{A review of Bayesian variable selection methods: what, how and which}},
url = {http://projecteuclid.org/euclid.ba/1340370391},
volume = {4},
year = {2009}
}
@article{Liu2014,
author = {Liu, Ruitao and Chakrabarti, Arijit and Samanta, Tapas and Ghosh, Jayanta K and Ghosh, Malay},
doi = {10.1214/14-BA862},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Liu et al/Bayesian Analysis/Liu et al. - 2014 - On divergence measures leading to Jeffreys and other reference priors.pdf:pdf},
issn = {19316690},
journal = {Bayesian Analysis},
keywords = {Jeffreys prior,Reference prior,$\alpha$-divergences},
number = {2},
pages = {331--370},
title = {{On divergence measures leading to Jeffreys and other reference priors}},
volume = {9},
year = {2014}
}
@article{Kass1994,
abstract = {Abstract Subjectivism has become the dominant philosophical foundation for Bayesian infer- ence. Yet, in practice, most Bayesian analyses are performed with so-called $\backslash$noninfor- mative" priors, that is, priors constructed by some formal rule. We review the plethora of ...},
author = {Kass, Robert E and Wasserman, Larry},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kass, Wasserman/Interpretation A Journal Of Bible And Theology/Kass, Wasserman - 1994 - Formal Rules for Selecting Prior Distributions A Review and Annotated Bibliography.pdf:pdf},
journal = {Interpretation A Journal Of Bible And Theology},
keywords = {and phrases,bayes factors,coherence,data-translated likelihoods,en-,fisher information,haar measure,improper priors,insu cient reason,je reys,marginalization paradoxes,noninformative priors,nuisance parameters,prior,priors,reference,s,sensitivity analysis,tropy},
pages = {1--81},
title = {{Formal Rules for Selecting Prior Distributions : A Review and Annotated Bibliography}},
year = {1994}
}
@article{Bernardo1979,
author = {Bernardo, Jose M},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Bernardo/Journal of Royal Statistical Socity/Bernardo - 1979 - Reference Posterior Distribution for Bayesian Inference (with Discussion).pdf:pdf},
journal = {Journal of Royal Statistical Socity},
number = {2},
pages = {113--147},
title = {{Reference Posterior Distribution for Bayesian Inference (with Discussion)}},
volume = {41},
year = {1979}
}
@article{Chipman2008,
author = {Chipman, Hugh and George, Edward I. and McCulloch, Robert E},
doi = {10.1214/193940307000000455},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Chipman, George, McCulloch/IMS Lecture Notes - Monograph Series/Chipman, George, McCulloch - 2001 - The practical implementation of bayesian model selection(2).pdf:pdf},
isbn = {978-0-940600-74-4},
journal = {IMS Lecture Notes - Monograph Series},
pages = {65--134},
title = {{The practical implementation of bayesian model selection}},
url = {http://projecteuclid.org/euclid.imsc/1207580085},
volume = {38},
year = {2001}
}
@article{Ong2004,
abstract = {In this paper we show that many kernel methods can be adapted to deal with indefinite kernels, that is, kernels which are not positive semidefinite. They do not satisfy Mercer‘s condition and they induce associated functional spaces called Reproducing Kernel Kre{\&}icaron;n Spaces (RKKS), a generalization of Reproducing Kernel Hilbert Spaces (RKHS).Machine learning in RKKS shares many "nice" properties of learning in RKHS, such as orthogonality and projection. However, since the kernels are indefinite, we can no longer minimize the loss, instead we stabilize it. We show a general representer theorem for constrained stabilization and prove generalization bounds by computing the Rademacher averages of the kernel class. We list several examples of indefinite kernels and investigate regularization methods to solve spline interpolation. Some preliminary experiments with indefinite kernels for spline smoothing are reported for truncated spectral factorization, Landweber-Fridman iterations, and MR-II.},
author = {Ong, Cheng Soon and Mary, Xavier and Canu, St{\'{e}}phane and Smola, Alex},
doi = {10.1145/1015330.1015443},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Ong et al/Proceedings of the Twenty-First International Conference on Machine Learning (ICML 2004)/Ong et al. - 2004 - Learning with Non-Positive Kernels.pdf:pdf},
isbn = {1581138385},
issn = {00200255},
journal = {Proceedings of the Twenty-First International Conference on Machine Learning (ICML 2004)},
keywords = {computational,information theoretic learning with statistics,learning,statistics {\&} optimisation,theory {\&} algorithms},
number = {7},
pages = {81},
pmid = {19487391},
title = {{Learning with Non-Positive Kernels}},
url = {http://eprints.pascal-network.org/archive/00000714/},
year = {2004}
}
@article{Jaynes1968,
abstract = {In decision theory, mathematical analysis shows that once the sampling distribution, loss function, and sample are specified, the only remaining basis for a choice among different admissible decisions lies in the prior probabilities. Therefore, the logical foundations of decision theory cannot be put in fully satisfactory form until the old problem of arbitrariness (sometimes called "subjectiveness") in assigning prior probabilities is resolved. The principle of maximum entropy represents one step in this direction. Its use is illustrated, and a correspondence property between maximum-entropy probabilities and frequencies is demonstrated. The consistency of this principle with the principles of conventional "direct probability" analysis is illustrated by showing that many known results may be derived by either method. However, an ambiguity remains in setting up a prior on a continuous parameter space because the results lack invariance under a change of parameters; thus a further principle is needed. It is shown that in many problems, including some of the most important in practice, this ambiguity can be removed by applying methods of group theoretical reasoning which have long been used in theoretical physics. By finding the group of transformations on the parameter space which convert the problem into an equivalent one, a basic desideratum of consistency can be stated in the form of functional equations which impose conditions on, and in some cases fully determine, an "invariant measure" on the parameter space.},
author = {Jaynes, Edwin T.},
doi = {10.1109/TSSC.1968.300117},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jaynes/IEEE Transactions on Systems Science and Cybernetics/Jaynes - 1968 - Prior Probabilities.pdf:pdf},
isbn = {0536-1567},
issn = {0536-1567},
journal = {IEEE Transactions on Systems Science and Cybernetics},
number = {3},
pages = {227--241},
title = {{Prior Probabilities}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4082152},
volume = {4},
year = {1968}
}
@article{Jeffreys1946,
abstract = {It is shown that a certain differential form depending on the values of the parameters in a law of chance is invariant for all transformations of the parameters when the law is differentiable with regard to all parameters. For laws containing a location and a scale parameter a form with a somewhat restricted type of invariance is found even when the law is not everywhere differentiable with regard to the parameters. This form has the properties required to give a general rule for stating the prior probability in a large class of estimation problems. CR - Copyright {\&}{\#}169; 1946 The Royal Society},
author = {Jeffreys, Harold},
doi = {10.1098/rspa.1946.0056},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jeffreys/Proceedings of the Royal Society of London. Series A Mathematical and physical sciences/Jeffreys - 1946 - An invariant form for the prior probability in estimation problems.pdf:pdf},
issn = {1364-5021},
journal = {Proceedings of the Royal Society of London. Series A: Mathematical and physical sciences},
keywords = {STATISTICS},
number = {1007},
pages = {453--461},
pmid = {20998741},
title = {{An invariant form for the prior probability in estimation problems.}},
volume = {186},
year = {1946}
}
@book{SAS2008,
address = {Cary, NC},
author = {{SAS Institute Inc.}},
edition = {2nd},
isbn = {978-1-60764-566-5},
publisher = {SAS Institute Inc.},
title = {{SAS/STAT(R) 9.2 User's Guide}},
url = {https://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm{\#}statug{\_}reg{\_}sect055.htm},
year = {2008}
}
@article{Sha2004,
abstract = {Here we focus on discrimination problems where the number of predictors substantially exceeds the sample size and we propose a Bayesian variable selection approach to multinomial probit models. Our method makes use of mixture priors and Markov chain Monte Carlo techniques to select sets of variables that differ among the classes. We apply our methodology to a problem in functional genomics using gene expression profiling data. The aim of the analysis is to identify molecular signatures that characterize two different stages of rheumatoid arthritis.},
author = {Sha, Naijun and Vannucci, Marina and Tadesse, Mahlet G. and Brown, Philip J. and Dragoni, Ilaria and Davies, Nick and Roberts, Tracy C. and Contestabile, Andrea and Salmon, Mike and Buckley, Chris and Falciani, Francesco},
doi = {10.1111/j.0006-341X.2004.00233.x},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Sha et al/Biometrics/Sha et al. - 2004 - Bayesian variable selection in multinomial probit models to identify molecular signatures of disease stage.pdf:pdf},
isbn = {0006-341X (Print)$\backslash$r0006-341X (Linking)},
issn = {0006341X},
journal = {Biometrics},
keywords = {Bayesian variable selection,DNA microarrays,Discrimination,Latent variables,MCMC,Multinomial probit model,Truncated sampling},
number = {3},
pages = {812--819},
pmid = {15339306},
title = {{Bayesian variable selection in multinomial probit models to identify molecular signatures of disease stage}},
volume = {60},
year = {2004}
}
@article{Fouskakis2009,
abstract = {In the field of quality of health care measurement, one approach to assessing patient sickness at admission involves a logistic regression of mortality within 30 days of admission on a fairly large number of sickness indicators (on the order of 100) to construct a sickness scale, employing classical variable selection methods to find an "optimal" subset of 10-20 indicators. Such "benefit-only" methods ignore the considerable differences among the sickness indicators in cost of data collection, an issue that is crucial when admission sickness is used to drive programs (now implemented or under consideration in several countries, including the U.S. and U.K.) that attempt to identify substandard hospitals by comparing observed and expected mortality rates (given admission sickness). When both data-collection cost and accuracy of prediction of 30-day mortality are considered, a large variable-selection problem arises in which costly variables that do not predict well enough should be omitted from the final scale. In this paper (a) we develop a method for solving this problem based on posterior model odds, arising from a prior distribution that (1) accounts for the cost of each variable and (2) results in a set of posterior model probabilities that corresponds to a generalized cost-adjusted version of the Bayesian information criterion (BIC), and (b) we compare this method with a decision-theoretic cost-benefit approach based on maximizing expected utility. We use reversible-jump Markov chain Monte Carlo (RJMCMC) methods to search the model space, and we check the stability of our findings with two variants of the MCMC model composition (MC3) algorithm. We find substantial agreement between the decision-theoretic and cost-adjusted-BIC methods; the latter provides a principled approach to performing a cost-benefit trade-off that avoids ambiguities in identification of an appropriate utility structure. Our cost-benefit approach results in a set of models with a noticeable reduction in cost and dimensionality, and only a minor decrease in predictive performance, when compared with models arising from benefit-only analyses.},
archivePrefix = {arXiv},
arxivId = {arXiv:0908.2313v1},
author = {Fouskakis, Dimitris and Ntzoufras, Ioannis and Draper, D.},
doi = {10.1214/08-AOAS207},
eprint = {arXiv:0908.2313v1},
file = {:Users/haziqjamil/Downloads/euclid.aoas.1245676190.pdf:pdf},
isbn = {1932-6157},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {Bayesian information criterion (BIC),Cost-adjusted BIC,Cost-benefit analysis,Input-output analysis,Laplace approximation,MCMC model composition (MC 3),Quality of health care,Reversible-jump Markov chain Monte Carlo (RJMCMC),Sickness at hospital admission},
number = {2},
pages = {663--690},
title = {{Bayesian variable selection using cost-adjusted BIC, with application to cost-effective measurement of quality of health care}},
volume = {3},
year = {2009}
}
@article{McDonald1973,
author = {McDonald, Gary C and Schwing, Richard C},
journal = {Technometrics},
number = {3},
pages = {463--481},
publisher = {Taylor {\&} Francis},
title = {{Instabilities of regression estimates relating air pollution to mortality}},
volume = {15},
year = {1973}
}
@article{George1993,
author = {George, Edward I and McCulloch, Robert E},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/George, McCulloch/Journal of the American Statistical Association/George, McCulloch - 1993 - Variable Selection Via Gibbs Sampling.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {423},
pages = {881--889},
title = {{Variable Selection Via Gibbs Sampling}},
url = {http://www.jstor.org/stable/2290777?seq=1{\#}page{\_}scan{\_}tab{\_}contents},
volume = {88},
year = {1993}
}
@article{Liang2008a,
abstract = {Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures.},
author = {Liang, F and Paulo, R and Molina, G and Clyde, M a and Berger, J O},
doi = {Doi 10.1198/016214507000001337},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Liang et al/Journal of the American Statistical Association/Liang et al. - 2008 - Mixtures of g priors for Bayesian variable selection(2).pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {aic,approximations,bayesian model averaging,bic,cauchy,criterion,empirical bayes,gaussian hypergeometric functions,linear-regression,matrix,model selection,multiple-regression,zellner-siow priors},
number = {481},
pages = {410--423},
title = {{Mixtures of g priors for Bayesian variable selection}},
volume = {103},
year = {2008}
}
@article{West2003,
abstract = {I discuss Bayesian factor regression models with many explanatory variables. These models are of particular interest and applicability in problems of prediction, but also for elucidating underlying structure in predictor variables. One key motivating application here is in studies of gene expression in functional genomics. I first discuss empirical factor (principal compo- nents) regression, and the use of general classes of shrinkage priors, with an example. These models raise foundational questions for Bayesians, and related practical issues, due to the use of design-dependent priors and the need to recover inferences on the effects of the original, high-dimensional predictors. I then discuss latent factor models for high-dimensional variables, and regression approaches in which low-dimensional latent factors are the predictor variables. These models generalise empirical factor regression, provide for more incisive evaluation of fac- tor structure underlying high-dimensional predictors, and resolve the modelling and practical issues in empirical factor models by casting the latter as limiting special cases. Finally, I turn to questions of prior specification in these models, and introduce sparse latent factor models to induce sparsity in factor loadings matrices. Embedding such sparse latent factor models in factor regressions provides a novel approach to variable selection with very many predictors. The paper concludes with an example of sparse factor analysis of gene expression data and comments about further research.},
author = {West, Mike},
doi = {10.1.1.18.3036},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/West/Bayesian Statistics 7 - Proceedings of the Seventh Valencia International Meeting/West - 2003 - Bayesian factor regression models in the “large p, small n” paradigm.pdf:pdf},
isbn = {978-0-19-852615-5},
issn = {08966273},
journal = {Bayesian Statistics 7 - Proceedings of the Seventh Valencia International Meeting},
keywords = {1,covariates,dimension reduction,empirical factor regression models,gene expression analysis,high-dimensional,latent factor models,shrinkage priors},
pages = {723--732},
pmid = {12495626},
title = {{Bayesian factor regression models in the “large p, small n” paradigm}},
url = {http://www.isds.duke.edu/courses/Spring06/sta376/Support/RegressionETC/v7.paper.pdf},
year = {2003}
}
@article{Ghosh2014,
abstract = {In this article, we highlight some interesting facts about Bayesian variable selection methods for linear regression models in settings where the design matrix exhibits strong collinearity. We first demonstrate via real data analysis and simulation studies that summaries of the posterior distribution based on marginal and joint distributions may give conflicting results for assessing the importance of strongly correlated covariates. The natural question is which one should be used in practice. The simulation studies suggest that posterior inclusion probabilities and Bayes factors that evaluate the importance of correlated covariates jointly are more appropriate, and some priors may be more adversely affected in such a setting. To obtain a better understanding behind the phenomenon, we study some toy examples with Zellner's g-prior. The results show that strong collinearity may lead to a multimodal posterior distribution over models, in which joint summaries are more appropriate than marginal summaries. Thus, we recommend a routine examination of the correlation matrix and calculation of the joint inclusion probabilities for correlated covariates, in addition to marginal inclusion probabilities, for assessing the importance of covariates in Bayesian variable selection.},
author = {Ghosh, Joyee and Ghattas, Andrew E},
doi = {10.1080/00031305.2015.1031827},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Ghosh, Ghattas/The American Statistician/Ghosh, Ghattas - 2015 - Bayesian variable selection under collinearity.pdf:pdf},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {bayesian model averaging,chain monte carlo,linear regression,marginal inclusion probability,markov,median probability model,multimodality,s g -prior,zellner},
number = {3},
pages = {165--173},
title = {{Bayesian variable selection under collinearity}},
volume = {69},
year = {2015}
}
@book{jaynes2003,
author = {Jaynes, Edwin T.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jaynes/Unknown/Jaynes - 2003 - Probability theory the logic of science.pdf:pdf},
publisher = {Cambridge university press},
title = {{Probability theory: the logic of science}},
year = {2003}
}
@article{alpay1991,
author = {Alpay, Daniel},
journal = {Rocky Mountain Journal of Mathematics},
number = {4},
pages = {1189--1205},
title = {{Some remarks on reproducing kernel Krein spaces}},
volume = {21},
year = {1991}
}
@article{Shmueli2010,
abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
archivePrefix = {arXiv},
arxivId = {1101.0891},
author = {Shmueli, Galit},
doi = {10.1214/10-STS330},
eprint = {1101.0891},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Shmueli/Statistical Science/Shmueli - 2010 - To Explain or to Predict.pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Explanatory modeling, causality, predictive modeli,and phrases,causality,data mining,eling,explanatory modeling,predictive mod-,predictive power,scientific research,statistical strategy},
number = {3},
pages = {289--310},
title = {{To Explain or to Predict?}},
url = {http://www.jstor.org/stable/41058949{\%}5Cnhttp://www.jstor.org/stable/pdfplus/10.2307/41058949.pdf?acceptTC=true},
volume = {25},
year = {2010}
}
@article{Jaynes1957,
abstract = {Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum-entropy estimate. it is the least biased estimate possible on the given information; i.e., it is maximally noncommital with regard to the missing information. If one considers statistical mechanics as a form of statistical inference rather than as a physical theory, it is found that the usual computational rules, starting with the determination of the partition function, are an immediate consequence of the maximum-entropy principle. In the resulting "subjective statistical mechanics", the usual rules are thus justified independently of any physical argument, and in particular independently of experimental verification; whether or not the results agree with experiment, they still represent the best estimates that cound have been made on the basis of the information available. It is concluded that statistical mechancis need not be regarded as a physical theory dependent for its validity on the truth of additional assumptions not contained in the laws of mechanics (such as ergodicity, metric transitivity, equal a priori probabilities, etc.). Furthermore, it is possible to maintain a sharp distinction between its physical and statistical aspects. The former consists only of the correct enumeration of the states of a system and their properties; the latter is a straightforward example of statistical inference.},
author = {Jaynes, Edwin T.},
doi = {10.1103/PhysRev.108.171},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Jaynes/Physical Review/Jaynes - 1957 - Information theory and statistical mechanics.pdf:pdf},
isbn = {1536-6065},
issn = {{\textless}null{\textgreater}},
journal = {Physical Review},
keywords = {information theory,statistical mechanics},
number = {4},
pages = {620--630},
pmid = {17798674},
title = {{Information theory and statistical mechanics}},
volume = {106},
year = {1957}
}
