Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@book{DasGupta2011,
abstract = {The exponential family is a practically convenient and widely used unified family of distributions on finite-dimensional Euclidean spaces parametrized by a finite-dimensional parameter vector. Specialized to the case of the real line, the exponential family contains as special cases most of the standard discrete and continuous distributions that we use for practical modeling, such as the normal, Poisson, binomial, exponential, Gamma, multivariate normal, and so on. The reason for the special status of the exponential family is that a number of important and useful calculations in statistics can be done all at one stroke within the framework of the exponential family. This generality contributes to both convenience and larger-scale understanding. The exponential family is the usual testing ground for the large spectrum of results in parametric statistical theory that require notions of regularity or Cram{\'{e}}r–Rao regularity. In addition, the unified calculations in the exponential family have an element of mathematical neatness. Distributions in the exponential family have been used in classical statistics for decades. However, it has recently obtained additional importance due to its use and appeal to the machine learning community. A fundamental treatment of the general exponential family is provided in this chapter. Classic expositions are available in Barndorff-Nielsen (1978), Brown (1986), and Lehmann and Casella (1998). An excellent recent treatment is available in Bickel and Doksum (2006).},
address = {New York, NY},
author = {DasGupta, Anirban},
doi = {10.1007/978-1-4419-9634-3},
isbn = {978-1-4419-9633-6},
publisher = {Springer New York},
series = {Springer Texts in Statistics},
title = {{Probability for Statistics and Machine Learning}},
url = {http://link.springer.com/10.1007/978-1-4419-9634-3},
year = {2011}
}
@book{Little2002,
abstract = {Emphasizes the latest trends in the field. Includes a new chapter on evolving methods. Provides updated or revised material in most of the chapters.},
author = {Little, Roderick J A and Rubin, Donald B},
booktitle = {Statistical analysis with missing data Second edition},
isbn = {0471183865},
pages = {408},
title = {{Statistical Analysis with Missing Data, Second Edition}},
year = {2002}
}
@article{Dellaportas2002,
abstract = {Several MCMC methods have been proposed for estimating probabilities of models and associated ‘model-averaged' posterior distributions in the presence of model uncertainty.We discuss, compare, develop and illustrate several of these methods, focussing on connections between them.},
author = {Dellaportas, Petros and Forster, Jonathan J. and Ntzoufras, Ioannis},
doi = {10.1023/A:1013164120801},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Dellaportas, Forster, Ntzoufras/Statistics and Computing/Dellaportas, Forster, Ntzoufras - 2002 - On Bayesian model and variable selection using MCMC.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Gibbs sampler,Independence sampler,Metropolis-Hastings,Reversible jump},
number = {1},
pages = {27--36},
title = {{On Bayesian model and variable selection using MCMC}},
volume = {12},
year = {2002}
}
@book{Ntzoufras2008,
author = {Ntzoufras, Ioannis},
booktitle = {Wiley},
doi = {10.1002/9780470434567.ch11},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Ntzoufras/Wiley/Ntzoufras - 2011 - Bayesian Modeling Using WinBUGS.pdf:pdf},
keywords = {Bayes factors,Monte Carlo estimators,harmonic mean estimators,marginal likelihood,prior predictive distributions},
pages = {389--433},
publisher = {Wiley},
title = {{Bayesian Modeling Using WinBUGS}},
year = {2011}
}
@article{Booth1999,
author = {Booth, J. G. and Hobert, J. P.},
doi = {10.1111/1467-9868.00176},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Booth, Hobert/Journal of the Royal Statistical Society Series B (Statistical Methodology)/Booth, Hobert - 1999 - Maximizing generalized linear mixed model likelihoods with an automated Monte Carlo EM algorithm.pdf:pdf},
issn = {1369-7412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {approximation,con,dence ellipsoid,hastings,importance sampling,laplace,markov chain monte carlo,method,metropolis algorithm,rejection sampling,salamander data,sandwich},
month = {feb},
number = {1},
pages = {265--285},
title = {{Maximizing generalized linear mixed model likelihoods with an automated Monte Carlo EM algorithm}},
url = {http://doi.wiley.com/10.1111/1467-9868.00176},
volume = {61},
year = {1999}
}
@article{Laird1982,
author = {Laird, NM and Ware, JH},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Laird, Ware/Biometrics/Laird, Ware - 1982 - Random-effects models for longitudinal data.pdf:pdf},
journal = {Biometrics},
number = {4},
pages = {963--974},
title = {{Random-effects models for longitudinal data}},
url = {http://www.jstor.org/stable/2529876},
volume = {38},
year = {1982}
}
@article{Kuo1998,
author = {Kuo, L and Mallick, B},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Kuo, Mallick/Sankhya The Indian Journal of Statistics, Series B/Kuo, Mallick - 1998 - Variable selection for regression models.pdf:pdf},
journal = {Sankhya: The Indian Journal of Statistics, Series B},
number = {1},
pages = {65--81},
title = {{Variable selection for regression models}},
volume = {60},
year = {1998}
}
@article{McCulloch1997,
author = {McCulloch, Charles E.},
doi = {10.1080/01621459.1997.10473613},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/McCulloch/Journal of the American Statistical Association/McCulloch - 1997 - Maximum Likelihood Algorithms for Generalized Linear Mixed Models.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {importance sampling,metropolis-hastings algorithm,monte carlo em,newton-raphson algorithm,penalized,quasi-likelihood,simulated maximum likelihood},
month = {mar},
number = {437},
pages = {162--170},
title = {{Maximum Likelihood Algorithms for Generalized Linear Mixed Models}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1997.10473613},
volume = {92},
year = {1997}
}
@article{Borman2009,
abstract = {This tutorial discusses the ExpectationMaximization (EM) algorithm of Demp- ster, Laird and Rubin 1. The approach taken follows that of an unpublished note by Stuart Russel, but fleshes out some of the gory details. In order to ensure that the presentation is reasonably self-contained, some of the results on which the derivation of the algorithm is based are presented prior to the main results. The EM algorithm has become a popular tool in statistical estimation problems involving incomplete data, or in problems which can be posed in a sim- ilar form, such as mixture estimation 3, 4. The EM algorithm has also been used in various motion estimation frameworks 5 and variants of it have been used in multiframe superresolution restoration methods which combine motion estimation along the lines of 2.},
author = {Borman, Sean},
doi = {10.1097/RLU.0b013e3181b06c41\r00003072-200909000-00002},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Borman/Submitted for publication/Borman - 2009 - The Expectation Maximization Algorithm A short tutorial.pdf:pdf},
issn = {15360229},
journal = {Submitted for publication},
pages = {1--9},
pmid = {19692813},
title = {{The Expectation Maximization Algorithm A short tutorial}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.150.8193{\&}rep=rep1{\&}type=pdf},
volume = {25},
year = {2009}
}
@article{Dempster1977,
abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
author = {Dempster, A.P. and Laird, N.M. and Rubin, D.B.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Dempster, Laird, Rubin/Journal of the Royal Statistical Society, Series B (Statistical Methodology)/Dempster, Laird, Rubin - 1977 - Maximum likelihood from incomplete data via the EM algorithm.pdf:pdf},
journal = {Journal of the Royal Statistical Society, Series B (Statistical Methodology)},
number = {1},
pages = {1--38},
title = {{Maximum likelihood from incomplete data via the EM algorithm}},
url = {http://www.jstor.org/stable/10.2307/2984875},
volume = {39},
year = {1977}
}
@misc{d1999using,
author = {D'Souza, Aaron A.},
title = {{Using EM to estimate a probablity density with a mixture of gaussians}},
year = {1999}
}
@article{George1993,
author = {George, Edward I and McCulloch, Robert E},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/George, McCulloch/Journal of the American Statistical Association/George, McCulloch - 1993 - Variable Selection Via Gibbs Sampling.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {423},
pages = {881--889},
title = {{Variable Selection Via Gibbs Sampling}},
url = {http://www.jstor.org/stable/2290777?seq=1{\#}page{\_}scan{\_}tab{\_}contents},
volume = {88},
year = {1993}
}
@article{StackOverflow,
author = {{Stack Overflow}},
journal = {http://stackoverflow.com/questions/2908822/speed-up-the-loop-operation-in-r},
title = {{Speed up the loop operation in R}},
url = {http://stackoverflow.com/questions/2908822/speed-up-the-loop-operation-in-r},
year = {2011}
}
@article{Steele1996,
author = {Steele, BM},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Steele/Biometrics/Steele - 1996 - A modified EM algorithm for estimation in generalized mixed models.pdf:pdf},
journal = {Biometrics},
number = {4},
pages = {1295--1310},
title = {{A modified EM algorithm for estimation in generalized mixed models}},
url = {http://www.jstor.org/stable/2532845},
volume = {52},
year = {1996}
}
@article{Liang2008,
abstract = {Zellner's g prior remains a popular conventional prior for use in Bayesian variable selection, despite several undesirable consistency issues. In this article we study mixtures of g priors as an alternative to default g priors that resolve many of the problems with the original formulation while maintaining the computational tractability that has made the g prior so popular. We present theoretical properties of the mixture g priors and provide real and simulated examples to compare the mixture formulation with fixed g priors, empirical Bayes approaches, and other default procedures.},
author = {Liang, F and Paulo, R and Molina, G and Clyde, M. and Berger, James O},
doi = {Doi 10.1198/016214507000001337},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Liang et al/Journal of the American Statistical Association/Liang et al. - 2008 - Mixtures of g priors for Bayesian variable selection.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {aic,approximations,bayesian model averaging,bic,cauchy,criterion,empirical bayes,gaussian hypergeometric functions,linear-regression,matrix,model selection,multiple-regression,zellner-siow priors},
number = {481},
pages = {410--423},
title = {{Mixtures of g priors for Bayesian variable selection}},
volume = {103},
year = {2008}
}
@article{Huber2004,
author = {Huber, Philippe and Ronchetti, Elvezio and Victoria-Feser, Maria-Pia},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Huber, Ronchetti, Victoria-Feser/Journal of the Royal Statistical Society Series B (Statistical Methodology)/Huber, Ronchetti, Victoria-Feser - 2004 - Estimation of generalized linear latent variable models.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
number = {4},
pages = {893--908},
title = {{Estimation of generalized linear latent variable models}},
url = {http://www.blackwell-synergy.com/doi/abs/10.1111/j.1467-9868.2004.05627.x},
volume = {66},
year = {2004}
}
@article{Lindstrom1988,
author = {Lindstrom, Mary J. and Bates, Douglas M.},
file = {:Users/haziqjamil/Documents/Mendeley Desktop/Lindstrom, Bates/Journal of the American {\ldots}/Lindstrom, Bates - 1988 - Newton Raphson and EM Algorithms for Linear Mixed Effects models for Repeated Measures Data.pdf:pdf},
journal = {Journal of the American {\ldots}},
number = {404},
pages = {1014--1022},
title = {{Newton Raphson and EM Algorithms for Linear Mixed Effects models for Repeated Measures Data}},
url = {http://amstat.tandfonline.com/doi/abs/10.1080/01621459.1988.10478693},
volume = {83},
year = {1988}
}
